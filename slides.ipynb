{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minimal-invitation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Radial Basis Function Network (RBFN) Tutorial\n",
    "\n",
    "Laura S. Mendoza\n",
    "\n",
    "<div style=\"text-align: right\"> Groupe de Travail ML </div>\n",
    "\n",
    "<div style=\"text-align: right\"> 14 avril 2020 </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-columbus",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "- Réseau de Neurones qui utilise des **fonctions à bases radiales** comme fonctions d'activation\n",
    "- Apprentissage supervisé\n",
    "- Utilisations : **Approximations de fonctions** (espaces à grandes dimensions), classifications, problèmes mal posés...\n",
    "- **Motivation** : utiliser les principes d'interpolation de fonctions à multiples variables dans le RN\n",
    "- Première formulation par Broomhead et Lowe en 88[[1]](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf), puis, Powel en 87[[2]](https://ci.nii.ac.jp/naid/10000092862/), Govindaraju R.S., Zhang B. (2000)[[3]](https://link.springer.com/chapter/10.1007/978-94-015-9341-0_6), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-poison",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fonction à base radiale\n",
    "- Une fonction à base radiale (RBF) est une fonction $\\phi$ symétrique autour d’un centre $x_i$ et avec une certaine étendue $\\sigma$ : $\\phi_j(x) = \\phi(||x− x_j||, \\sigma)$, avec $||.||$ la norme éuclidienne.\n",
    "- Les RBF les plus utilisées: gaussiennes de même ampleur (1)\n",
    "\n",
    "$$\\phi(x) = \\exp({-\\beta||x-x_i||^2})$$\n",
    "\n",
    "<img src=\"images/diff_variances_plot.png\" align=\"center\"/>\n",
    "\n",
    "\n",
    "<sub><sup>[source image](http://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/)</sup></sub>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "accessible-exhibition",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principe\n",
    "<img src=\"images/architecture_simple2.png\" align=\"center\"/>\n",
    "\n",
    "Un RBFN est composé de 3 couches: entrée, **une** couche cachée, sortie\n",
    "\n",
    "\n",
    "<sub><sup>[source image](http://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/)</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-static",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principe\n",
    "\n",
    "3 couches :\n",
    "- **input** : vecteur de dimension $n$, tout le vecteur est donné à chacun des noeuds\n",
    "- **neurones RBF** : ont chacune un *prototype* (vecteur du training set, le *centre* des RBF), retourne une valeur entre $0$ (le plus éloigné) et $1$ (le plus proche du prototype), activation.\n",
    "- **output** : vecteur des *scores* $F$ pour chaque catégorie (souvent : positif si dans la catégorie, négatif sinon)\n",
    "\n",
    "$$ F(x) = \\sum_{i=0}^N \\omega_i \\phi_i(||x− x_j||, \\sigma)$$\n",
    "\n",
    "avec $N$ le nombre neurones et $\\omega_i$ les poids associés à chaque noeud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-tower",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparaison aux réseaux MLP (Multi-layer Perceptron)\n",
    "\n",
    "- Plus rapide et facile (pas besoin de back-propagation pour la phase d'apprentissage)\n",
    "- Sens clair à chaque noeud du réseaux\n",
    "- Pas besoin de calculer le nombre noeuds ou de couches cachées\n",
    "- Mauvaise pour les données bruitées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-creativity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Phase d'apprentissage\n",
    "\n",
    "Il y a 4 paramètres à déterminer :\n",
    "\n",
    "- Le nombre de neurones RBF.\n",
    "- La position des centres des gaussiennes de chacun des neurones.\n",
    "- La largeur de ces gaussiennes.\n",
    "- Le poids des connexions entre les neurones RBF et les neurones de sortie.\n",
    "\n",
    "Apprentissage séquentiel de chacun des paramètres (Moody, 89[[5]](https://www.scirp.org/(S(czeh2tfqyw2orz553k1w0r45))/reference/ReferencesPapers.aspx?ReferenceID=2125361))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-knowing",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Phase d'apprentissage\n",
    "\n",
    "- L’apprentissage est souvent hybride :  \n",
    "    - non  supervisé  pour l’étape de construction du réseau\n",
    "    - supervisé pour la détermination des poids de la couche de sortie.\n",
    "- Notamment 3 schémas utilisés, décrits par Schwenker et al.[[4]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.312&rep=rep1&type=pdf) : à une, deux et trois phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-entity",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Nombre de RBF : pas de méthode, au choix, plus de neurones => plus c'est lent et précis. Particulier à chaque problème\n",
    "- Séléction prototypes (centres RBF) : aléatoire, **k-moyennes** clustering, quantification vectorielle et arbres de décision, ...\n",
    "- Largeur gaussienne: plusieurs formules directes utilisées, dépendent des prototypes\n",
    "- Apprentissage des poids : gradient conjugué, pseudo inversement.\n",
    "\n",
    "$ \\begin{pmatrix}\n",
    "\\varphi_1(||x_1 - c_1||) & ... & \\varphi_M(||x_1 - c_M||) \\\\\n",
    "\\varphi_1(||x_2 - c_1||) & ... & \\varphi_M(||x_2 - c_M||) \\\\\n",
    "... & ... & ... \\\\\n",
    "\\varphi_1(||x_N - c_1||) & ... & \\varphi_M(||x_N - c_M||) \n",
    "\\end{pmatrix}   \\begin{pmatrix}\n",
    "\\omega_1 \\\\\n",
    "\\omega_2 \\\\\n",
    "... \\\\\n",
    "\\omega_N \n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "... \\\\\n",
    "y_N \n",
    "\\end{pmatrix} $\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "leading-lightweight",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemple d'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-transcription",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/cluster_centers1.png\" align=\"center\"/>\n",
    "\n",
    "<sub><sup>[source image](http://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/)</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-killer",
   "metadata": {},
   "source": [
    "## Algorithme\n",
    "\n",
    "- 2 passages de **k-moyennes** (k=10) sur les 2 catégories des données d'entrainement => 20 centres de clusters\n",
    "- Résultat : prototypes des RBFs (centres)\n",
    "- Définir l'ampleur $\\sigma$ de la RBF à la moyenne de la distance entre le centre du cluster et chaque donnée du cluster\n",
    "$$ \\sigma = \\dfrac{1}{m}\\sum_{i=1}^m || x_i - c ||  $$\n",
    "$$ \\beta = \\dfrac{1}{2\\sigma^2}$$\n",
    "$$\\phi(x) = \\exp({-\\beta||x-c||^2})$$\n",
    "\n",
    "- On calcule les poids d'approximation $\\omega_i$ par un gradient conjugué (sur les valeurs de $\\phi(x)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-tours",
   "metadata": {},
   "source": [
    "<img src=\"images/category_1_scores_w_prototypes.png\" align=\"center\"/>\n",
    "\n",
    "<sub><sup>[source image](http://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/)</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-hungary",
   "metadata": {},
   "source": [
    "## Exemple : XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fossil-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Creation data\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
    "                     np.linspace(-3, 3, 50))\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(200, 2)\n",
    "Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
    "\n",
    "def rbf(x, c, s):\n",
    "    return np.exp(-1 / (2 * s**2) * (x-c)**2)\n",
    "\n",
    "# Calcul des centres\n",
    "n_neuro = 10\n",
    "kmeans = KMeans(n_clusters=n_neuro, random_state=0).fit(X)\n",
    "centers = kmeans.cluster_centers_\n",
    "sig = np.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-earthquake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
